{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "54c31017fb6f46769b5a899cec7142f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fe995d46aff6417faa830c2c671a2eb0",
              "IPY_MODEL_80d160dc5bfa491cb0da35fa80fa0089",
              "IPY_MODEL_e4ccb7cd950048dda4fcd6420b51d709"
            ],
            "layout": "IPY_MODEL_7829edb2da7c4458a9b39510397c4141"
          }
        },
        "fe995d46aff6417faa830c2c671a2eb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d1e837d1df94398986abc00b4b806df",
            "placeholder": "​",
            "style": "IPY_MODEL_35629779d7b64dec950b4160ed67688d",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "80d160dc5bfa491cb0da35fa80fa0089": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9ab641adfae548659e458a8e7855f3a3",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_466f87029016462ea6610bb19d3fd7f1",
            "value": 4
          }
        },
        "e4ccb7cd950048dda4fcd6420b51d709": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c29a66da28054c84b6f036ddbc155ea0",
            "placeholder": "​",
            "style": "IPY_MODEL_3be9fd5f805045c8adf70f0e1c6e591f",
            "value": " 4/4 [00:04&lt;00:00,  1.04s/it]"
          }
        },
        "7829edb2da7c4458a9b39510397c4141": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d1e837d1df94398986abc00b4b806df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35629779d7b64dec950b4160ed67688d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9ab641adfae548659e458a8e7855f3a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "466f87029016462ea6610bb19d3fd7f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c29a66da28054c84b6f036ddbc155ea0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3be9fd5f805045c8adf70f0e1c6e591f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LUMJ_e4L_uc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ! pip install poppler-utils\n",
        "!apt-get install poppler-utils\n",
        "! pip install pdf2image\n",
        "! pip install docling_core\n",
        "! pip install docling_document\n",
        "! pip install flash-attn --no-build-isolation"
      ],
      "metadata": {
        "id": "FaZVQTfJS6PA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c732bc35-b827-4635-98af-91b003b0735f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "poppler-utils is already the newest version (22.02.0-2ubuntu0.7).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 34 not upgraded.\n",
            "Requirement already satisfied: pdf2image in /usr/local/lib/python3.11/dist-packages (1.17.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from pdf2image) (11.1.0)\n",
            "Requirement already satisfied: docling_core in /usr/local/lib/python3.11/dist-packages (2.27.0)\n",
            "Requirement already satisfied: jsonref<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from docling_core) (1.1.0)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.16.0 in /usr/local/lib/python3.11/dist-packages (from docling_core) (4.23.0)\n",
            "Requirement already satisfied: latex2mathml<4.0.0,>=3.77.0 in /usr/local/lib/python3.11/dist-packages (from docling_core) (3.77.0)\n",
            "Requirement already satisfied: pandas<3.0.0,>=2.1.4 in /usr/local/lib/python3.11/dist-packages (from docling_core) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0.0,>=10.0.0 in /usr/local/lib/python3.11/dist-packages (from docling_core) (11.1.0)\n",
            "Requirement already satisfied: pydantic!=2.10.0,!=2.10.1,!=2.10.2,<3.0.0,>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from docling_core) (2.11.3)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.1 in /usr/local/lib/python3.11/dist-packages (from docling_core) (6.0.2)\n",
            "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from docling_core) (0.9.0)\n",
            "Requirement already satisfied: typer<0.16.0,>=0.12.5 in /usr/local/lib/python3.11/dist-packages (from docling_core) (0.15.2)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from docling_core) (4.13.1)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.16.0->docling_core) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.16.0->docling_core) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.16.0->docling_core) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.16.0->docling_core) (0.24.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>=2.1.4->docling_core) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>=2.1.4->docling_core) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>=2.1.4->docling_core) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>=2.1.4->docling_core) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=2.10.0,!=2.10.1,!=2.10.2,<3.0.0,>=2.6.0->docling_core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic!=2.10.0,!=2.10.1,!=2.10.2,<3.0.0,>=2.6.0->docling_core) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=2.10.0,!=2.10.1,!=2.10.2,<3.0.0,>=2.6.0->docling_core) (0.4.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<0.16.0,>=0.12.5->docling_core) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<0.16.0,>=0.12.5->docling_core) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<0.16.0,>=0.12.5->docling_core) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0.0,>=2.1.4->docling_core) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<0.16.0,>=0.12.5->docling_core) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<0.16.0,>=0.12.5->docling_core) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<0.16.0,>=0.12.5->docling_core) (0.1.2)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement docling_document (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for docling_document\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting flash-attn\n",
            "  Downloading flash_attn-2.7.4.post1.tar.gz (6.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from flash-attn) (2.6.0+cu124)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from flash-attn) (0.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->flash-attn)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->flash-attn)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->flash-attn)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->flash-attn)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->flash-attn)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->flash-attn)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->flash-attn)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->flash-attn)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->flash-attn)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->flash-attn)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->flash-attn) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->flash-attn) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m111.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: flash-attn\n",
            "  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flash-attn: filename=flash_attn-2.7.4.post1-cp311-cp311-linux_x86_64.whl size=187831595 sha256=58853b28a5a926cae14402bfd8d4d93a45ebf8f9e79533f37ab09d0d77a99c05\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/88/d8/284b89f56af7d5bf366b10d6b8e251ac8a7c7bf3f04203fb4f\n",
            "Successfully built flash-attn\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, flash-attn\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed flash-attn-2.7.4.post1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "nvidia"
                ]
              },
              "id": "da45633a08fd4a8ba2b4e3741b4d1778"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "IHZNuf67jE5k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "54c31017fb6f46769b5a899cec7142f2",
            "fe995d46aff6417faa830c2c671a2eb0",
            "80d160dc5bfa491cb0da35fa80fa0089",
            "e4ccb7cd950048dda4fcd6420b51d709",
            "7829edb2da7c4458a9b39510397c4141",
            "8d1e837d1df94398986abc00b4b806df",
            "35629779d7b64dec950b4160ed67688d",
            "9ab641adfae548659e458a8e7855f3a3",
            "466f87029016462ea6610bb19d3fd7f1",
            "c29a66da28054c84b6f036ddbc155ea0",
            "3be9fd5f805045c8adf70f0e1c6e591f"
          ]
        },
        "outputId": "6fb5de1c-2b35-49ab-cf79-cac15797d72c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total GPU memory: 39.56 GB\n",
            "Loading model from OpenGVLab/InternVL2-8B\n",
            "Warning: Flash attention is not available, using eager attention instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "54c31017fb6f46769b5a899cec7142f2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting PDF /content/SHIMI.pdf to images...\n",
            "Converted 17 pages to images\n",
            "Model is on device: cuda:0, dtype: torch.bfloat16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing pages:   0%|          | 0/17 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing page 1/17\n",
            "Pixel values dtype: torch.bfloat16, device: cuda:0\n",
            "Running prompt 1/3\n",
            "Running prompt 2/3\n",
            "Running prompt 3/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing pages:   6%|▌         | 1/17 [00:49<13:15, 49.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing page 2/17\n",
            "Pixel values dtype: torch.bfloat16, device: cuda:0\n",
            "Running prompt 1/3\n",
            "Running prompt 2/3\n",
            "Running prompt 3/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing pages:  12%|█▏        | 2/17 [01:40<12:37, 50.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing page 3/17\n",
            "Pixel values dtype: torch.bfloat16, device: cuda:0\n",
            "Running prompt 1/3\n",
            "Running prompt 2/3\n",
            "Running prompt 3/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing pages:  18%|█▊        | 3/17 [02:10<09:31, 40.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing page 4/17\n",
            "Pixel values dtype: torch.bfloat16, device: cuda:0\n",
            "Running prompt 1/3\n",
            "Running prompt 2/3\n",
            "Running prompt 3/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing pages:  24%|██▎       | 4/17 [02:42<08:09, 37.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing page 5/17\n",
            "Pixel values dtype: torch.bfloat16, device: cuda:0\n",
            "Running prompt 1/3\n",
            "Running prompt 2/3\n",
            "Running prompt 3/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing pages:  29%|██▉       | 5/17 [03:52<09:51, 49.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing page 6/17\n",
            "Pixel values dtype: torch.bfloat16, device: cuda:0\n",
            "Running prompt 1/3\n",
            "Running prompt 2/3\n",
            "Running prompt 3/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing pages:  35%|███▌      | 6/17 [05:22<11:32, 62.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing page 7/17\n",
            "Pixel values dtype: torch.bfloat16, device: cuda:0\n",
            "Running prompt 1/3\n",
            "Running prompt 2/3\n",
            "Running prompt 3/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing pages:  41%|████      | 7/17 [06:26<10:34, 63.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing page 8/17\n",
            "Pixel values dtype: torch.bfloat16, device: cuda:0\n",
            "Running prompt 1/3\n",
            "Running prompt 2/3\n",
            "Running prompt 3/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing pages:  47%|████▋     | 8/17 [07:00<08:04, 53.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing page 9/17\n",
            "Pixel values dtype: torch.bfloat16, device: cuda:0\n",
            "Running prompt 1/3\n",
            "Running prompt 2/3\n",
            "Running prompt 3/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing pages:  53%|█████▎    | 9/17 [07:58<07:22, 55.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing page 10/17\n",
            "Pixel values dtype: torch.bfloat16, device: cuda:0\n",
            "Running prompt 1/3\n",
            "Running prompt 2/3\n",
            "Running prompt 3/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing pages:  59%|█████▉    | 10/17 [08:57<06:35, 56.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing page 11/17\n",
            "Pixel values dtype: torch.bfloat16, device: cuda:0\n",
            "Running prompt 1/3\n",
            "Running prompt 2/3\n",
            "Running prompt 3/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing pages:  65%|██████▍   | 11/17 [09:26<04:48, 48.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing page 12/17\n",
            "Pixel values dtype: torch.bfloat16, device: cuda:0\n",
            "Running prompt 1/3\n",
            "Running prompt 2/3\n",
            "Running prompt 3/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing pages:  71%|███████   | 12/17 [09:59<03:37, 43.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing page 13/17\n",
            "Pixel values dtype: torch.bfloat16, device: cuda:0\n",
            "Running prompt 1/3\n",
            "Running prompt 2/3\n",
            "Running prompt 3/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing pages:  76%|███████▋  | 13/17 [10:26<02:33, 38.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing page 14/17\n",
            "Pixel values dtype: torch.bfloat16, device: cuda:0\n",
            "Running prompt 1/3\n",
            "Running prompt 2/3\n",
            "Running prompt 3/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing pages:  82%|████████▏ | 14/17 [11:16<02:05, 41.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing page 15/17\n",
            "Pixel values dtype: torch.bfloat16, device: cuda:0\n",
            "Running prompt 1/3\n",
            "Running prompt 2/3\n",
            "Running prompt 3/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing pages:  88%|████████▊ | 15/17 [12:00<01:25, 42.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing page 16/17\n",
            "Pixel values dtype: torch.bfloat16, device: cuda:0\n",
            "Running prompt 1/3\n",
            "Running prompt 2/3\n",
            "Running prompt 3/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing pages:  94%|█████████▍| 16/17 [13:23<00:54, 54.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing page 17/17\n",
            "Pixel values dtype: torch.bfloat16, device: cuda:0\n",
            "Running prompt 1/3\n",
            "Running prompt 2/3\n",
            "Running prompt 3/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing pages: 100%|██████████| 17/17 [13:41<00:00, 48.35s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to /content/results\n",
            "Saving model to /content/saved_model\n",
            "Model saved to /content/saved_model\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ! pip install poppler-utils\n",
        "!apt-get install poppler-utils\n",
        "! pip install pdf2image\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision.transforms as T\n",
        "from PIL import Image\n",
        "from torchvision.transforms.functional import InterpolationMode\n",
        "from transformers import AutoModel, AutoTokenizer, BitsAndBytesConfig\n",
        "import os\n",
        "from pdf2image import convert_from_path\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
        "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
        "\n",
        "def build_transform(input_size):\n",
        "    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD\n",
        "    transform = T.Compose([\n",
        "        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
        "        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(mean=MEAN, std=STD)\n",
        "    ])\n",
        "    return transform\n",
        "\n",
        "def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n",
        "    best_ratio_diff = float('inf')\n",
        "    best_ratio = (1, 1)\n",
        "    area = width * height\n",
        "    for ratio in target_ratios:\n",
        "        target_aspect_ratio = ratio[0] / ratio[1]\n",
        "        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n",
        "        if ratio_diff < best_ratio_diff:\n",
        "            best_ratio_diff = ratio_diff\n",
        "            best_ratio = ratio\n",
        "        elif ratio_diff == best_ratio_diff:\n",
        "            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n",
        "                best_ratio = ratio\n",
        "    return best_ratio\n",
        "\n",
        "def dynamic_preprocess(image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):\n",
        "    orig_width, orig_height = image.size\n",
        "    aspect_ratio = orig_width / orig_height\n",
        "    target_ratios = set(\n",
        "        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1)\n",
        "        if i * j <= max_num and i * j >= min_num)\n",
        "    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n",
        "    target_aspect_ratio = find_closest_aspect_ratio(\n",
        "        aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n",
        "    target_width = image_size * target_aspect_ratio[0]\n",
        "    target_height = image_size * target_aspect_ratio[1]\n",
        "    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n",
        "    resized_img = image.resize((target_width, target_height))\n",
        "    processed_images = []\n",
        "    for i in range(blocks):\n",
        "        box = (\n",
        "            (i % (target_width // image_size)) * image_size,\n",
        "            (i // (target_width // image_size)) * image_size,\n",
        "            ((i % (target_width // image_size)) + 1) * image_size,\n",
        "            ((i // (target_width // image_size)) + 1) * image_size\n",
        "        )\n",
        "        split_img = resized_img.crop(box)\n",
        "        processed_images.append(split_img)\n",
        "    assert len(processed_images) == blocks\n",
        "    if use_thumbnail and len(processed_images) != 1:\n",
        "        thumbnail_img = image.resize((image_size, image_size))\n",
        "        processed_images.append(thumbnail_img)\n",
        "    return processed_images\n",
        "\n",
        "def load_image(image_file, input_size=448, max_num=12, dtype=torch.float32, device='cpu'):\n",
        "    image = Image.open(image_file).convert('RGB')\n",
        "    transform = build_transform(input_size=input_size)\n",
        "    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
        "    pixel_values = [transform(image) for image in images]\n",
        "    pixel_values = torch.stack(pixel_values).to(dtype=dtype, device=device)\n",
        "    return pixel_values\n",
        "\n",
        "def load_model(model_path, use_gpu=True, device_map=\"auto\", load_in_8bit=False, load_in_4bit=False):\n",
        "    \"\"\"Load the InternVL model and tokenizer with memory optimization options.\"\"\"\n",
        "    print(f\"Loading model from {model_path}\")\n",
        "\n",
        "    # Create BitsAndBytesConfig if 4-bit or 8-bit quantization is requested\n",
        "    quantization_config = None\n",
        "    if load_in_4bit:\n",
        "        print(\"Loading model in 4-bit quantization mode\")\n",
        "        quantization_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_compute_dtype=torch.float16,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_use_double_quant=True\n",
        "        )\n",
        "    elif load_in_8bit:\n",
        "        print(\"Loading model in 8-bit quantization mode\")\n",
        "        quantization_config = BitsAndBytesConfig(\n",
        "            load_in_8bit=True\n",
        "        )\n",
        "\n",
        "    # Memory optimization settings\n",
        "    kwargs = {\n",
        "        \"low_cpu_mem_usage\": True,\n",
        "        \"trust_remote_code\": True,\n",
        "    }\n",
        "\n",
        "    # Set dtype based on device\n",
        "    dtype = torch.bfloat16 if use_gpu and torch.cuda.is_available() else torch.float32\n",
        "    kwargs[\"torch_dtype\"] = dtype\n",
        "\n",
        "    # Add device_map if specified\n",
        "    if device_map is not None:\n",
        "        kwargs[\"device_map\"] = device_map\n",
        "\n",
        "    # Add quantization config if specified\n",
        "    if quantization_config is not None:\n",
        "        kwargs[\"quantization_config\"] = quantization_config\n",
        "\n",
        "    try:\n",
        "        # Load model\n",
        "        model = AutoModel.from_pretrained(model_path, **kwargs)\n",
        "\n",
        "        # If not using device_map, explicitly move to appropriate device\n",
        "        if device_map is None:\n",
        "            if use_gpu and torch.cuda.is_available():\n",
        "                print(\"Moving model to CUDA\")\n",
        "                model = model.cuda()\n",
        "            else:\n",
        "                print(\"Using CPU for inference (warning: this will be very slow)\")\n",
        "                model = model.cpu()\n",
        "\n",
        "        model = model.eval()  # Set to evaluation mode\n",
        "\n",
        "    except RuntimeError as e:\n",
        "        if \"CUDA out of memory\" in str(e):\n",
        "            print(\"GPU out of memory error detected. Trying with more aggressive memory optimization...\")\n",
        "            if load_in_8bit:\n",
        "                print(\"Falling back to 4-bit quantization\")\n",
        "                return load_model(model_path, use_gpu, device_map, False, True)\n",
        "            elif not load_in_8bit and not load_in_4bit:\n",
        "                print(\"Falling back to 8-bit quantization\")\n",
        "                return load_model(model_path, use_gpu, device_map, True, False)\n",
        "            else:\n",
        "                print(\"Falling back to CPU\")\n",
        "                return load_model(model_path, False, \"cpu\", False, False)\n",
        "        else:\n",
        "            raise e\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True, use_fast=False)\n",
        "    return model, tokenizer, dtype\n",
        "\n",
        "def convert_pdf_to_images(pdf_path, output_dir, dpi=300):\n",
        "    \"\"\"Convert PDF pages to images.\"\"\"\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    print(f\"Converting PDF {pdf_path} to images...\")\n",
        "    images = convert_from_path(pdf_path, dpi=dpi)\n",
        "\n",
        "    image_paths = []\n",
        "    for i, image in enumerate(images):\n",
        "        image_path = os.path.join(output_dir, f'page_{i+1}.jpg')\n",
        "        image.save(image_path, 'JPEG')\n",
        "        image_paths.append(image_path)\n",
        "\n",
        "    print(f\"Converted {len(image_paths)} pages to images\")\n",
        "    return image_paths\n",
        "\n",
        "def process_pdf_with_vlm(model, tokenizer, pdf_path, output_dir, save_dir, max_num=12, use_gpu=True, batch_size=1, dtype=torch.float32):\n",
        "    \"\"\"Process a PDF file with the VLM model.\"\"\"\n",
        "    # Create image directory\n",
        "    image_dir = os.path.join(output_dir, 'images')\n",
        "    if not os.path.exists(image_dir):\n",
        "        os.makedirs(image_dir)\n",
        "\n",
        "    # Convert PDF to images\n",
        "    image_paths = convert_pdf_to_images(pdf_path, image_dir)\n",
        "\n",
        "    # Determine device\n",
        "    device = torch.device(\"cuda\" if use_gpu and torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Model is on device: {next(model.parameters()).device}, dtype: {dtype}\")\n",
        "\n",
        "    # Process each page with the VLM\n",
        "    results = []\n",
        "    for i, image_path in enumerate(tqdm(image_paths, desc=\"Processing pages\")):\n",
        "        page_num = i + 1\n",
        "        print(f\"Processing page {page_num}/{len(image_paths)}\")\n",
        "\n",
        "        try:\n",
        "            # Load and prepare the image\n",
        "            pixel_values = load_image(image_path, max_num=max_num, dtype=dtype, device=device)\n",
        "            print(f\"Pixel values dtype: {pixel_values.dtype}, device: {pixel_values.device}\")\n",
        "\n",
        "            # Clear CUDA cache between pages to prevent memory buildup\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "            # Generate prompts for different aspects of the document\n",
        "            prompts = [\n",
        "                \"<image>\\nExtract the hierarchical structure of this document page, identifying all headings, subheadings, and their content. Format the output as structured text maintaining the hierarchy.\",\n",
        "                \"<image>\\nIdentify and extract any tables, lists, or structured elements in this document page.\",\n",
        "                \"<image>\\nExtract any figures, charts, or visual elements on this page and describe their content.\"\n",
        "            ]\n",
        "\n",
        "            page_results = {}\n",
        "            for j, prompt in enumerate(prompts):\n",
        "                print(f\"Running prompt {j+1}/{len(prompts)}\")\n",
        "                generation_config = dict(max_new_tokens=1024, do_sample=False)\n",
        "\n",
        "                # Process with smaller chunks if memory issues occur\n",
        "                try:\n",
        "                    response = model.chat(tokenizer, pixel_values, prompt, generation_config)\n",
        "                except RuntimeError as e:\n",
        "                    if \"CUDA out of memory\" in str(e):\n",
        "                        print(\"CUDA out of memory when processing prompt. Falling back to CPU...\")\n",
        "                        if torch.cuda.is_available():\n",
        "                            torch.cuda.empty_cache()\n",
        "                        model_cpu = model.cpu()\n",
        "                        pixel_values_cpu = pixel_values.cpu().to(torch.float32)  # Use float32 on CPU\n",
        "\n",
        "                        # Process in smaller batches if needed\n",
        "                        if pixel_values_cpu.size(0) > 1:\n",
        "                            print(f\"Processing in smaller batches (original batch size: {pixel_values_cpu.size(0)})\")\n",
        "                            responses = []\n",
        "                            for k in range(0, pixel_values_cpu.size(0), batch_size):\n",
        "                                batch = pixel_values_cpu[k:k+batch_size]\n",
        "                                batch_response = model_cpu.chat(tokenizer, batch, prompt, generation_config)\n",
        "                                responses.append(batch_response)\n",
        "                            response = \" \".join(responses)\n",
        "                        else:\n",
        "                            response = model_cpu.chat(tokenizer, pixel_values_cpu, prompt, generation_config)\n",
        "\n",
        "                        # Move model back to GPU if it was there before\n",
        "                        if use_gpu and torch.cuda.is_available():\n",
        "                            model.cuda()\n",
        "                    else:\n",
        "                        raise e\n",
        "\n",
        "                if j == 0:\n",
        "                    page_results[\"structure\"] = response\n",
        "                elif j == 1:\n",
        "                    page_results[\"tables_lists\"] = response\n",
        "                elif j == 2:\n",
        "                    page_results[\"figures\"] = response\n",
        "\n",
        "            page_results[\"page_number\"] = page_num\n",
        "            results.append(page_results)\n",
        "\n",
        "            # Clear memory after each page\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing page {page_num}: {str(e)}\")\n",
        "            page_results = {\n",
        "                \"page_number\": page_num,\n",
        "                \"error\": str(e),\n",
        "                \"structure\": \"Error during processing\",\n",
        "                \"tables_lists\": \"Error during processing\",\n",
        "                \"figures\": \"Error during processing\"\n",
        "            }\n",
        "            results.append(page_results)\n",
        "\n",
        "    # Save results\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "\n",
        "    # Save as JSON\n",
        "    json_path = os.path.join(save_dir, \"pdf_parsed_results.json\")\n",
        "    with open(json_path, 'w') as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "\n",
        "    # Save as text\n",
        "    text_path = os.path.join(save_dir, \"pdf_parsed_results.txt\")\n",
        "    with open(text_path, 'w') as f:\n",
        "        for page in results:\n",
        "            f.write(f\"PAGE {page['page_number']}\\n\")\n",
        "            f.write(\"=\"*80 + \"\\n\\n\")\n",
        "            f.write(\"DOCUMENT STRUCTURE:\\n\")\n",
        "            f.write(page.get('structure', 'Not available') + \"\\n\\n\")\n",
        "            f.write(\"TABLES AND LISTS:\\n\")\n",
        "            f.write(page.get('tables_lists', 'Not available') + \"\\n\\n\")\n",
        "            f.write(\"FIGURES AND VISUALS:\\n\")\n",
        "            f.write(page.get('figures', 'Not available') + \"\\n\\n\")\n",
        "            f.write(\"-\"*80 + \"\\n\\n\")\n",
        "\n",
        "    print(f\"Results saved to {save_dir}\")\n",
        "    return results\n",
        "\n",
        "def save_model_local(model, tokenizer, save_dir):\n",
        "    \"\"\"Save the fine-tuned model to a local directory.\"\"\"\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "\n",
        "    print(f\"Saving model to {save_dir}\")\n",
        "    model.save_pretrained(save_dir)\n",
        "    tokenizer.save_pretrained(save_dir)\n",
        "    print(f\"Model saved to {save_dir}\")\n",
        "\n",
        "def main():\n",
        "    # Hardcoded parameters\n",
        "    pdf_path = \"/content/SHIMI.pdf\"  # Adjusted for Databricks DBFS\n",
        "    model_path = \"OpenGVLab/InternVL2-8B\"\n",
        "    output_dir = \"/content/output\"\n",
        "    save_dir = \"/content/results\"\n",
        "    model_save_dir = \"/content/saved_model\"\n",
        "    max_num = 6  # Reduced from 12 to lower memory requirements\n",
        "\n",
        "    # Memory optimization settings\n",
        "    use_gpu = True  # Set to False to force CPU usage\n",
        "    load_in_8bit = False  # Set to True to use 8-bit quantization\n",
        "    load_in_4bit = False  # Set to True to use 4-bit quantization\n",
        "    device_map = \"auto\"  # Helps with model distribution across multiple GPUs or CPU\n",
        "    batch_size = 1  # For batch processing when falling back to smaller chunks\n",
        "\n",
        "    # Determine available VRAM to guide loading strategy\n",
        "    if use_gpu and torch.cuda.is_available():\n",
        "        free_mem = torch.cuda.get_device_properties(0).total_memory / (1024**3)  # GB\n",
        "        print(f\"Total GPU memory: {free_mem:.2f} GB\")\n",
        "\n",
        "        # Adjust settings based on available memory\n",
        "        if free_mem < 10:  # Less than 10GB\n",
        "            print(\"Low GPU memory detected, using 4-bit quantization\")\n",
        "            load_in_4bit = True\n",
        "            max_num = 2  # Further reduce max tiles\n",
        "        elif free_mem < 16:  # Less than 16GB\n",
        "            print(\"Medium GPU memory detected, using 8-bit quantization\")\n",
        "            load_in_8bit = True\n",
        "            max_num = 4  # Reduce max tiles\n",
        "\n",
        "    # Load model with memory optimization\n",
        "    try:\n",
        "        model, tokenizer, dtype = load_model(\n",
        "            model_path,\n",
        "            use_gpu=use_gpu,\n",
        "            device_map=device_map,\n",
        "            load_in_8bit=load_in_8bit,\n",
        "            load_in_4bit=load_in_4bit\n",
        "        )\n",
        "\n",
        "        # Process PDF with memory management\n",
        "        process_pdf_with_vlm(\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            pdf_path=pdf_path,\n",
        "            output_dir=output_dir,\n",
        "            save_dir=save_dir,\n",
        "            max_num=max_num,\n",
        "            use_gpu=use_gpu,\n",
        "            batch_size=batch_size,\n",
        "            dtype=dtype\n",
        "        )\n",
        "\n",
        "        # Save model\n",
        "        try:\n",
        "            save_model_local(model, tokenizer, model_save_dir)\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not save model locally due to: {e}\")\n",
        "            print(\"Skipping model saving step.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        print(\"Try reducing max_num further or setting use_gpu=False to use CPU mode.\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PW7Sm6gcSYmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "neVDJsCRfvd4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5Oq9o5R5fvaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FWPFJJ6OfvYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ! pip install poppler-utils\n",
        "!apt-get install poppler-utils\n",
        "! pip install pdf2image\n",
        "! pip install docling_core\n",
        "# ! pip install docling_document\n",
        "! pip install flash-attn --no-build-isolation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrtbWNnZfvVK",
        "outputId": "17c5d2b4-1aed-49ce-ed4a-6e5066f0c8ac"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 34 not upgraded.\n",
            "Need to get 186 kB of archives.\n",
            "After this operation, 696 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.7 [186 kB]\n",
            "Fetched 186 kB in 2s (119 kB/s)\n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 126332 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.7_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.7) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.7) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Collecting pdf2image\n",
            "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from pdf2image) (11.1.0)\n",
            "Downloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: pdf2image\n",
            "Successfully installed pdf2image-1.17.0\n",
            "Collecting docling_core\n",
            "  Downloading docling_core-2.27.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting jsonref<2.0.0,>=1.1.0 (from docling_core)\n",
            "  Downloading jsonref-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.16.0 in /usr/local/lib/python3.11/dist-packages (from docling_core) (4.23.0)\n",
            "Collecting latex2mathml<4.0.0,>=3.77.0 (from docling_core)\n",
            "  Downloading latex2mathml-3.77.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: pandas<3.0.0,>=2.1.4 in /usr/local/lib/python3.11/dist-packages (from docling_core) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0.0,>=10.0.0 in /usr/local/lib/python3.11/dist-packages (from docling_core) (11.1.0)\n",
            "Requirement already satisfied: pydantic!=2.10.0,!=2.10.1,!=2.10.2,<3.0.0,>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from docling_core) (2.11.3)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.1 in /usr/local/lib/python3.11/dist-packages (from docling_core) (6.0.2)\n",
            "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from docling_core) (0.9.0)\n",
            "Requirement already satisfied: typer<0.16.0,>=0.12.5 in /usr/local/lib/python3.11/dist-packages (from docling_core) (0.15.2)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from docling_core) (4.13.1)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.16.0->docling_core) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.16.0->docling_core) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.16.0->docling_core) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.16.0->docling_core) (0.24.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>=2.1.4->docling_core) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>=2.1.4->docling_core) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>=2.1.4->docling_core) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>=2.1.4->docling_core) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=2.10.0,!=2.10.1,!=2.10.2,<3.0.0,>=2.6.0->docling_core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic!=2.10.0,!=2.10.1,!=2.10.2,<3.0.0,>=2.6.0->docling_core) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=2.10.0,!=2.10.1,!=2.10.2,<3.0.0,>=2.6.0->docling_core) (0.4.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<0.16.0,>=0.12.5->docling_core) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<0.16.0,>=0.12.5->docling_core) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<0.16.0,>=0.12.5->docling_core) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0.0,>=2.1.4->docling_core) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<0.16.0,>=0.12.5->docling_core) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<0.16.0,>=0.12.5->docling_core) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<0.16.0,>=0.12.5->docling_core) (0.1.2)\n",
            "Downloading docling_core-2.27.0-py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.8/133.8 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonref-1.1.0-py3-none-any.whl (9.4 kB)\n",
            "Downloading latex2mathml-3.77.0-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.7/73.7 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: latex2mathml, jsonref, docling_core\n",
            "Successfully installed docling_core-2.27.0 jsonref-1.1.0 latex2mathml-3.77.0\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement docling_document (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for docling_document\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting flash-attn\n",
            "  Downloading flash_attn-2.7.4.post1.tar.gz (6.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m115.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from flash-attn) (2.6.0+cu124)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from flash-attn) (0.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->flash-attn)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->flash-attn)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->flash-attn)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->flash-attn)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->flash-attn)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->flash-attn)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->flash-attn)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->flash-attn)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->flash-attn)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->flash-attn)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->flash-attn) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->flash-attn) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m125.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m98.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m109.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: flash-attn\n",
            "  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flash-attn: filename=flash_attn-2.7.4.post1-cp311-cp311-linux_x86_64.whl size=187831595 sha256=58853b28a5a926cae14402bfd8d4d93a45ebf8f9e79533f37ab09d0d77a99c05\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/88/d8/284b89f56af7d5bf366b10d6b8e251ac8a7c7bf3f04203fb4f\n",
            "Successfully built flash-attn\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, flash-attn\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed flash-attn-2.7.4.post1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "from pdf2image import convert_from_path\n",
        "from docling_core.types.doc import DoclingDocument\n",
        "from docling_core.types.doc.document import DocTagsDocument\n",
        "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
        "from pathlib import Path\n",
        "\n",
        "class PDFParser:\n",
        "    def __init__(self, model_name=\"ds4sd/SmolDocling-256M-preview\", device=None):\n",
        "        \"\"\"\n",
        "        Initialize the PDF parser with SmolDocling model\n",
        "\n",
        "        Args:\n",
        "            model_name (str): Name of the model to use\n",
        "            device (str): Device to use for inference (cuda or cpu)\n",
        "        \"\"\"\n",
        "        if device is None:\n",
        "            self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        else:\n",
        "            self.device = device\n",
        "\n",
        "        print(f\"Using device: {self.device}\")\n",
        "\n",
        "        # Initialize processor and model\n",
        "        self.processor = AutoProcessor.from_pretrained(model_name)\n",
        "\n",
        "        # First initialize model without flash attention\n",
        "        self.model = AutoModelForVision2Seq.from_pretrained(\n",
        "            model_name,\n",
        "            torch_dtype=torch.bfloat16,\n",
        "        )\n",
        "\n",
        "        # Then move model to device\n",
        "        self.model = self.model.to(self.device)\n",
        "\n",
        "        # Finally set attention implementation if using CUDA\n",
        "        if self.device == \"cuda\":\n",
        "            try:\n",
        "                # Try to set flash attention after moving to GPU\n",
        "                self.model.config.attn_implementation = \"flash_attention_2\"\n",
        "                print(\"Successfully enabled Flash Attention 2.0\")\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Could not enable Flash Attention 2.0: {e}\")\n",
        "                print(\"Falling back to standard attention implementation\")\n",
        "\n",
        "    def convert_pdf_to_images(self, pdf_path, dpi=300):\n",
        "        \"\"\"\n",
        "        Convert PDF to list of PIL images\n",
        "\n",
        "        Args:\n",
        "            pdf_path (str): Path to the PDF file\n",
        "            dpi (int): DPI for the output images\n",
        "\n",
        "        Returns:\n",
        "            list: List of PIL images\n",
        "        \"\"\"\n",
        "        print(f\"Converting PDF to images: {pdf_path}\")\n",
        "        return convert_from_path(pdf_path, dpi=dpi)\n",
        "\n",
        "    def process_image(self, image):\n",
        "        \"\"\"\n",
        "        Process a single image with SmolDocling\n",
        "\n",
        "        Args:\n",
        "            image (PIL.Image): Image to process\n",
        "\n",
        "        Returns:\n",
        "            tuple: (doctags, docling_document)\n",
        "        \"\"\"\n",
        "        # Create input messages\n",
        "        messages = [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"image\"},\n",
        "                    {\"type\": \"text\", \"text\": \"Convert this page to docling.\"}\n",
        "                ]\n",
        "            },\n",
        "        ]\n",
        "\n",
        "        # Prepare inputs\n",
        "        prompt = self.processor.apply_chat_template(messages, add_generation_prompt=True)\n",
        "        inputs = self.processor(\n",
        "            text=prompt,\n",
        "            images=[image],\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True\n",
        "        )\n",
        "        inputs = inputs.to(self.device)\n",
        "\n",
        "        # Generate outputs\n",
        "        generated_ids = self.model.generate(**inputs, max_new_tokens=8192)\n",
        "        prompt_length = inputs.input_ids.shape[1]\n",
        "        trimmed_generated_ids = generated_ids[:, prompt_length:]\n",
        "        doctags = self.processor.batch_decode(\n",
        "            trimmed_generated_ids,\n",
        "            skip_special_tokens=False,\n",
        "        )[0].lstrip()\n",
        "\n",
        "        # Create doctags document\n",
        "        doctags_doc = DocTagsDocument.from_doctags_and_image_pairs([doctags], [image])\n",
        "\n",
        "        # Create docling document\n",
        "        doc = DoclingDocument(name=\"Page\")\n",
        "        doc.load_from_doctags(doctags_doc)\n",
        "\n",
        "        return doctags, doc\n",
        "\n",
        "    def process_pdf(self, pdf_path, output_dir=None):\n",
        "        \"\"\"\n",
        "        Process a PDF file and save results\n",
        "\n",
        "        Args:\n",
        "            pdf_path (str): Path to the PDF file\n",
        "            output_dir (str): Directory to save output files\n",
        "\n",
        "        Returns:\n",
        "            dict: Combined JSON result for all pages\n",
        "        \"\"\"\n",
        "        # Verify PDF path exists\n",
        "        if not os.path.exists(pdf_path):\n",
        "            raise FileNotFoundError(f\"PDF file not found: {pdf_path}\")\n",
        "\n",
        "        # Create output directory if it doesn't exist\n",
        "        if output_dir is None:\n",
        "            pdf_name = os.path.basename(pdf_path).replace('.pdf', '')\n",
        "            output_dir = f\"output_{pdf_name}\"\n",
        "\n",
        "        Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        try:\n",
        "            # Convert PDF to images\n",
        "            images = self.convert_pdf_to_images(pdf_path)\n",
        "            if not images:\n",
        "                raise ValueError(f\"Failed to extract images from PDF: {pdf_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error converting PDF to images: {e}\")\n",
        "            raise\n",
        "\n",
        "        # Process each page\n",
        "        combined_result = {\n",
        "            \"document_name\": os.path.basename(pdf_path),\n",
        "            \"pages\": []\n",
        "        }\n",
        "\n",
        "        # Create a metadata file with processing information\n",
        "        with open(os.path.join(output_dir, \"metadata.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "            metadata = {\n",
        "                \"source_pdf\": pdf_path,\n",
        "                \"total_pages\": len(images),\n",
        "                \"model_used\": self.model.__class__.__name__,\n",
        "                \"device\": self.device,\n",
        "                \"processing_date\": str(Path(output_dir).stat().st_mtime)\n",
        "            }\n",
        "            json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        # Keep track of successful and failed pages\n",
        "        successful_pages = 0\n",
        "        failed_pages = []\n",
        "\n",
        "        for i, image in enumerate(images):\n",
        "            page_num = i + 1\n",
        "            print(f\"Processing page {page_num}/{len(images)}\")\n",
        "\n",
        "            try:\n",
        "                doctags, doc = self.process_image(image)\n",
        "\n",
        "                # Save doctags\n",
        "                with open(os.path.join(output_dir, f\"page_{page_num}_doctags.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "                    f.write(doctags)\n",
        "\n",
        "                successful_pages += 1\n",
        "            except Exception as e:\n",
        "                error_message = f\"Error processing page {page_num}: {str(e)}\"\n",
        "                print(error_message)\n",
        "                failed_pages.append({\"page\": page_num, \"error\": error_message})\n",
        "\n",
        "                # Save error information\n",
        "                with open(os.path.join(output_dir, f\"page_{page_num}_error.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "                    f.write(f\"Error processing page {page_num}: {str(e)}\")\n",
        "\n",
        "                # Create a placeholder for this page\n",
        "                doctags = \"<doc></doc>\"\n",
        "                doc = DoclingDocument(name=f\"Page {page_num} (Error)\")\n",
        "\n",
        "                # Continue with the next page\n",
        "                continue\n",
        "\n",
        "            # Save markdown\n",
        "            markdown = doc.export_to_markdown()\n",
        "            with open(os.path.join(output_dir, f\"page_{page_num}.md\"), \"w\", encoding=\"utf-8\") as f:\n",
        "                f.write(markdown)\n",
        "\n",
        "            # Get JSON representation by converting from markdown\n",
        "            # First export to markdown\n",
        "            markdown = doc.export_to_markdown()\n",
        "\n",
        "            # Create a simplified JSON structure from the markdown content\n",
        "            # This is a basic representation - you might need to customize based on your needs\n",
        "            blocks = []\n",
        "            lines = markdown.split('\\n')\n",
        "            current_block = {\"type\": \"paragraph\", \"content\": \"\"}\n",
        "\n",
        "            for line in lines:\n",
        "                # Skip empty lines\n",
        "                if not line.strip():\n",
        "                    if current_block[\"content\"]:\n",
        "                        blocks.append(current_block)\n",
        "                        current_block = {\"type\": \"paragraph\", \"content\": \"\"}\n",
        "                    continue\n",
        "\n",
        "                # Check for headings\n",
        "                if line.startswith('#'):\n",
        "                    if current_block[\"content\"]:\n",
        "                        blocks.append(current_block)\n",
        "\n",
        "                    # Count heading level\n",
        "                    level = 0\n",
        "                    for char in line:\n",
        "                        if char == '#':\n",
        "                            level += 1\n",
        "                        else:\n",
        "                            break\n",
        "\n",
        "                    current_block = {\n",
        "                        \"type\": f\"heading{level}\",\n",
        "                        \"content\": line[level:].strip()\n",
        "                    }\n",
        "                    blocks.append(current_block)\n",
        "                    current_block = {\"type\": \"paragraph\", \"content\": \"\"}\n",
        "                # Handle lists\n",
        "                elif line.strip().startswith(('- ', '* ', '1. ')):\n",
        "                    if current_block[\"type\"] != \"list\":\n",
        "                        if current_block[\"content\"]:\n",
        "                            blocks.append(current_block)\n",
        "                        current_block = {\"type\": \"list\", \"content\": line.strip() + \"\\n\"}\n",
        "                    else:\n",
        "                        current_block[\"content\"] += line.strip() + \"\\n\"\n",
        "                # Regular paragraph content\n",
        "                else:\n",
        "                    if current_block[\"type\"] != \"paragraph\":\n",
        "                        if current_block[\"content\"]:\n",
        "                            blocks.append(current_block)\n",
        "                        current_block = {\"type\": \"paragraph\", \"content\": line.strip()}\n",
        "                    else:\n",
        "                        if current_block[\"content\"]:\n",
        "                            current_block[\"content\"] += \" \" + line.strip()\n",
        "                        else:\n",
        "                            current_block[\"content\"] = line.strip()\n",
        "\n",
        "            # Add the last block if it has content\n",
        "            if current_block[\"content\"]:\n",
        "                blocks.append(current_block)\n",
        "\n",
        "            # Create the final JSON structure\n",
        "            json_data = {\n",
        "                \"blocks\": blocks,\n",
        "                \"raw_markdown\": markdown\n",
        "            }\n",
        "\n",
        "            with open(os.path.join(output_dir, f\"page_{page_num}.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "                json.dump(json_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "            # Add to combined result\n",
        "            combined_result[\"pages\"].append({\n",
        "                \"page_number\": page_num,\n",
        "                \"content\": json_data\n",
        "            })\n",
        "\n",
        "        # Update metadata with processing results\n",
        "        metadata_path = os.path.join(output_dir, \"metadata.json\")\n",
        "        if os.path.exists(metadata_path):\n",
        "            with open(metadata_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                metadata = json.load(f)\n",
        "        else:\n",
        "            metadata = {}\n",
        "\n",
        "        # Add processing statistics\n",
        "        metadata.update({\n",
        "            \"successful_pages\": successful_pages,\n",
        "            \"failed_pages\": len(failed_pages),\n",
        "            \"failed_pages_details\": failed_pages,\n",
        "            \"completion_time\": str(Path(output_dir).stat().st_mtime)\n",
        "        })\n",
        "\n",
        "        # Save updated metadata\n",
        "        with open(metadata_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        # Save combined JSON\n",
        "        combined_json_path = os.path.join(output_dir, \"combined_result.json\")\n",
        "        with open(combined_json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(combined_result, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        # Print summary\n",
        "        print(f\"\\nProcessing complete. Results saved to {output_dir}\")\n",
        "        print(f\"Combined JSON saved to {combined_json_path}\")\n",
        "        print(f\"Successfully processed {successful_pages} out of {len(images)} pages\")\n",
        "        if failed_pages:\n",
        "            print(f\"Failed to process {len(failed_pages)} pages. See metadata.json for details.\")\n",
        "\n",
        "        return combined_result\n",
        "\n",
        "def process_pdf_file(pdf_path, output_dir=None, model_name=\"ds4sd/SmolDocling-256M-preview\"):\n",
        "    \"\"\"\n",
        "    Utility function to process a PDF file\n",
        "\n",
        "    Args:\n",
        "        pdf_path (str): Path to the PDF file\n",
        "        output_dir (str): Directory to save output files\n",
        "        model_name (str): Name of the model to use\n",
        "\n",
        "    Returns:\n",
        "        dict: Combined JSON result for all pages\n",
        "    \"\"\"\n",
        "    parser = PDFParser(model_name=model_name)\n",
        "    return parser.process_pdf(pdf_path, output_dir)\n",
        "\n",
        "# # Example usage\n",
        "# if __name__ == \"__main__\":\n",
        "#     import argparse\n",
        "\n",
        "#     parser = argparse.ArgumentParser(description=\"Parse PDF using SmolDocling\")\n",
        "#     parser.add_argument(\"pdf_path\", help=\"Path to the PDF file\")\n",
        "#     parser.add_argument(\"--output-dir\", help=\"Directory to save output files\")\n",
        "#     parser.add_argument(\"--model\", default=\"ds4sd/SmolDocling-256M-preview\", help=\"Model name\")\n",
        "\n",
        "#     args = parser.parse_args()\n",
        "\n",
        "#     process_pdf_file(args.pdf_path, args.output_dir, args.model)\n",
        "\n",
        "# Prerequisites:\n",
        "# pip install torch\n",
        "# pip install docling_core\n",
        "# pip install transformers\n",
        "# pip install pdf2image\n",
        "# pip install poppler-utils (for Linux/WSL) or brew install poppler (for macOS)\n",
        "\n",
        "import os\n",
        "import json\n",
        "\n",
        "# Method 1: Simple function call (recommended for most cases)\n",
        "def process_single_pdf():\n",
        "    # Process a single PDF file\n",
        "    result = process_pdf_file(\n",
        "        pdf_path=\"/content/SHIMI.pdf\",\n",
        "        output_dir=\"/content/output\"  # Optional, will create a default folder if not specified\n",
        "    )\n",
        "\n",
        "    # Print structure of the first page to verify\n",
        "    if result[\"pages\"]:\n",
        "        print(\"\\nFirst page structure:\")\n",
        "        print(json.dumps(result[\"pages\"][0], indent=2))\n",
        "\n",
        "    return result\n",
        "\n",
        "# Method 2: Using the PDFParser class directly (for more control)\n",
        "def process_with_custom_options():\n",
        "    # Initialize the parser\n",
        "    parser = PDFParser(model_name=\"ds4sd/SmolDocling-256M-preview\")\n",
        "\n",
        "    # Process multiple PDFs\n",
        "    results = []\n",
        "    pdf_directory = \"/content\"\n",
        "    output_base_dir = \"/content/output\"\n",
        "\n",
        "    for filename in os.listdir(pdf_directory):\n",
        "        if filename.lower().endswith('.pdf'):\n",
        "            pdf_path = os.path.join(pdf_directory, filename)\n",
        "            output_dir = os.path.join(output_base_dir, os.path.splitext(filename)[0])\n",
        "\n",
        "            print(f\"\\nProcessing {filename}...\")\n",
        "            try:\n",
        "                result = parser.process_pdf(pdf_path, output_dir)\n",
        "                results.append({\"pdf\": filename, \"success\": True, \"result\": result})\n",
        "                print(f\"Successfully processed {filename}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {filename}: {e}\")\n",
        "                results.append({\"pdf\": filename, \"success\": False, \"error\": str(e)})\n",
        "\n",
        "    return results\n",
        "\n",
        "# Method 3: Command-line usage\n",
        "# Run from terminal:\n",
        "# python -m pdf_parser /path/to/your/document.pdf --output-dir /path/to/output --model ds4sd/SmolDocling-256M-preview\n",
        "\n",
        "# Choose one of the methods above or run directly as a script\n",
        "if __name__ == \"__main__\":\n",
        "    # Uncomment the method you want to use\n",
        "    # process_single_pdf()\n",
        "    process_with_custom_options()\n",
        "\n",
        "    # Or use command-line arguments if run as a script\n",
        "    # import argparse\n",
        "\n",
        "    # parser = argparse.ArgumentParser(description=\"Parse PDF using SmolDocling\")\n",
        "    # parser.add_argument(\"pdf_path\", help=\"Path to the PDF file\")\n",
        "    # parser.add_argument(\"--output-dir\", help=\"Directory to save output files\")\n",
        "    # parser.add_argument(\"--model\", default=\"ds4sd/SmolDocling-256M-preview\", help=\"Model name\")\n",
        "\n",
        "    # args = parser.parse_args()\n",
        "\n",
        "    # process_pdf_file(args.pdf_path, args.output_dir, args.model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4zlinsCfvR1",
        "outputId": "48143ff9-35bf-4845-b9e5-2476ac0657cc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Successfully enabled Flash Attention 2.0\n",
            "\n",
            "Processing SHIMI.pdf...\n",
            "Converting PDF to images: /content/SHIMI.pdf\n",
            "Processing page 1/17\n",
            "Processing page 2/17\n",
            "Processing page 3/17\n",
            "Processing page 4/17\n",
            "Processing page 5/17\n",
            "Processing page 6/17\n",
            "Processing page 7/17\n",
            "Processing page 8/17\n",
            "Processing page 9/17\n",
            "Processing page 10/17\n",
            "Processing page 11/17\n",
            "Processing page 12/17\n",
            "Processing page 13/17\n",
            "Processing page 14/17\n",
            "Processing page 15/17\n",
            "Processing page 16/17\n",
            "Processing page 17/17\n",
            "\n",
            "Processing complete. Results saved to /content/output/SHIMI\n",
            "Combined JSON saved to /content/output/SHIMI/combined_result.json\n",
            "Successfully processed 17 out of 17 pages\n",
            "Successfully processed SHIMI.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mmcjAyyvpNX9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}